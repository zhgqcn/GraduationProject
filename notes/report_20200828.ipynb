{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 神经网络"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import torch \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch.nn.functional as F\nimport torch.nn as nn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Net(torch.nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 6, 5)  # 1 input image channel, 6 output channels, 5*5 square convolution kernel\n        self.conv2 = nn.Conv2d(6, 16, 5) \n        self.fc1 = nn.Linear(16*5*5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n        \n    def forward(self, x):\n        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n        x = F.max_pool2d(F.relu(self.conv2(x)), (2, 2))\n        x = x.view(-1, self.num_flat_features(x))\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n    \n    def num_flat_features(self, x):\n        size = x.size()[1:]  # x.size() - (batchsize, channels, height, width), [1:]切片为（c, h, w）\n        num_features = 1\n        for s in size:\n            num_features *= s\n        return num_features\n\nnet = Net()\nprint(net)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = list(net.parameters())\nprint(params)\nprint(params[0].size())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input = torch.randn(1, 1, 32, 32)  # (bachsize, channels, height,width)\nout = net(input)\nprint(out)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"net.zero_grad() # 将所有参数的梯度缓存清零\nout.backward(torch.rand(1, 10)) # 随机梯度的反向传播","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 损失函数"},{"metadata":{"trusted":true},"cell_type":"code","source":"output = net(input)\ntarget = torch.randn(10)\ntarget = target.view(1, -1)\ncriterion = nn.MSELoss()\n\nloss = criterion(output, target)\nprint(loss)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n现在，如果在反向过程中跟随loss ， 使用它的 .grad_fn 属性，将看到如下所示的计算图。\n\ninput -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d\n      -> view -> linear -> relu -> linear -> relu -> linear\n      -> MSELoss\n      -> loss\n\n所以，当我们调用 loss.backward()时,整张计算图都会 根据loss进行微分，而且图中所有设置为requires_grad=True的张量 将会拥有一个随着梯度累积的.grad 张量。"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(loss.grad_fn)  # MSELoss\nprint(loss.grad_fn.next_functions[0][0])  # Linear\nprint(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 反向传播"},{"metadata":{"trusted":true},"cell_type":"code","source":"net.zero_grad()  # 调用前需要清除已存在的梯度，否则梯度将被累加到已存在的梯度\n \nprint('conv1.bias.grad before backward')\nprint(net.conv1.bias.grad)\n\nloss.backward()  # 获得反向传播的误差\n\nprint('conv1.bias.grad after backward')\nprint(net.conv1.bias.grad)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 更新权重"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.optim as optim\n\noptimizer = optim.SGD(net.parameters(), lr = 0.01)\n\noptimizer.zero_grad()\noutput = net(input)\nloss = criterion(output, target)\nloss.backward()\noptimizer.step()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 训练一个分类器\n1. 使用torchvision加载和归一化CIFAR10训练集和测试集\n2. 定义一个卷积神经网络\n3. 定义损失函数\n4. 在训练集上训练网络\n5. 在测试集上测试网络"},{"metadata":{},"cell_type":"markdown","source":"## 1.读取和归一化CIFAR10数据集"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torchvision\nimport torchvision.transforms as transforms","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transform = transforms.Compose(                 # 参数为list列表\n    [\n        transforms.ToTensor(),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # 均值 标准差 image = (image - mean) / std\n    ]\n)\n\ntrainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n                                        download=True, transform=transform)\n\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n                                          shuffle=True, num_workers=2)\n\ntestset = torchvision.datasets.CIFAR10(root='./data', train=False,\n                                       download=True, transform=transform)\n\ntestloader = torch.utils.data.DataLoader(testset, batch_size=4,\n                                         shuffle=False, num_workers=2)\n\nclasses = ('plane', 'car', 'bird', 'cat','deer', 'dog', 'frog', 'horse', 'ship', 'truck')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nimport numpy as np\n\ndef imshow(img):\n    img = img / 2 + 0.5    # 去正则化  image = ((image * std) + mean)\n    np_img = img.numpy()\n    plt.imshow(np.transpose(np_img, (1, 2, 0)))   # tranpose做转置\n    \n# 获取随机数据   \ndataiter = iter(trainloader)      # 每次迭代取的是一个batch\nimages, labels = dataiter.next()  # 如果batch_size为4，则取出来的images是4×c×h×w的tensor，labels是1×4的向量\nprint(images.size())\n\n# 展示图片\nimshow(torchvision.utils.make_grid(images)) # make_grid将若干图拼成一张\n\nprint(''.join('%10s' % classes[labels[j]] for j in range(4)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## 定义一个卷积神经网络\n从之前的神经网络一节复制神经网络代码，并修改为输入3通道图像"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n        \n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 16 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n    \nnet = Net()\nprint(net)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 定义损失函数和优化器"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.optim as optim\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr = 0.001, momentum=0.9)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 训练网络"},{"metadata":{"trusted":true},"cell_type":"code","source":"for epoch in range(2):\n    \n    running_loss = 0.0\n                                             \n    for i, data in enumerate(trainloader, 0): # the return of enumerate is dictory\n        # get inputs data\n        inputs, labels = data    # the type of data is list\n\n        # set gradient to 0\n        optimizer.zero_grad()\n\n        # forward, backward, optimize\n        outputs = net(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        # print info\n        running_loss += loss.item()\n        if i % 2000 == 1999: \n            print('[%d %5d] loss:%0.3f' % (epoch + 1, i + 1, running_loss / 2000))\n            running_loss = 0.0\n\nprint('Finish Training')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 测试"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataiter = iter(trainloader)\nimages, labels = dataiter.next()\n\nimshow(torchvision.utils.make_grid(images))\nprint('GT: ', ' '.join('%6s' % classes[labels[j]] for j in range(4)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"outputs = net(images)  # 通过网络进行预测\nprint(outputs)\nprint(outputs.data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_, predicted = torch.max(outputs, 1)  # the return of torch.max is [values, indies]\nprint(predicted)\nprint('Predicted: ', ' '.join('%5s' % classes[predicted[j]] for j in range(4)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 测试整个测试集\ncorrect = 0.0\ntotal = 0\n\nwith torch.no_grad():\n    for data in testloader:\n        images, labels = data\n        outputs = net(images)\n        _, predicted = torch.max(outputs.data, 1)  # 1 为找行的最大值，0为列\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n        \nprint(\"Acc of the net on the 10000 test imgs : %d %%\" % (100 * correct / total)) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"在识别哪一个类的时候好，哪一个不好呢？"},{"metadata":{"trusted":true},"cell_type":"code","source":"class_correct = list(0. for i in range(10))\nclass_total = list(0. for i in range(10))\nwith torch.no_grad():\n    for data in testloader:\n        images, labels = data\n        outputs = net(images)\n        _, predicted = torch.max(outputs, 1)\n        c = (predicted == labels).squeeze()\n        for i in range(4):\n            label = labels[i]\n            class_correct[label] += c[i].item()\n            class_total[label] += 1\n\n\nfor i in range(10):\n    print('Accuracy of %5s : %2d %%' % (\n        classes[i], 100 * class_correct[i] / class_total[i]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# MNIST数据集手写数字识别"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, models, transforms","execution_count":11,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 设置超函数"},{"metadata":{"trusted":true},"cell_type":"code","source":"Batch_size = 512\nEpochs = 20\nDevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # 设置GPU训练","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_loader = torch.utils.data.DataLoader(\n    datasets.MNIST('data', train=True, download=True,\n                  transform=transforms.Compose([\n                      transforms.ToTensor(),\n                      transforms.Normalize((0.1307, ), (0.3081, ))  # 归一化\n                  ])),\n    batch_size=Batch_size, shuffle=True\n)\n\n\ntest_loader = torch.utils.data.DataLoader(\n    datasets.MNIST('data', train=False, \n                  transform=transforms.Compose([\n                      transforms.ToTensor(),\n                      transforms.Normalize((0.1307, ), (0.3081, ))\n                  ])),\n    batch_size=Batch_size, shuffle=True\n)","execution_count":8,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 网络计算\noutput = (n - f + 2p) / s + 1 \n- n 是输入的大小\n- f 是卷积核大小\n- p 是padding大小\n- s 是步长"},{"metadata":{"trusted":true},"cell_type":"code","source":"class MNIST_NET(nn.Module):\n    def __init__(self):\n        super(MNIST_NET, self).__init__()\n        self.conv1 = nn.Conv2d(1, 10, 5) # 输入， 输出， 卷积核大小 \n        self.conv2 = nn.Conv2d(10, 20, 3)\n        self.fc1 = nn.Linear(20 * 10 * 10, 500)\n        self.fc2 = nn.Linear(500, 10)\n    \n    def forward(self, x):\n        in_size = x.size(0)  # x 的输入为（512,1,1,28）, 所以in_size = 512\n        x = F.max_pool2d(F.relu(self.conv1(x)), 2, 2)\n        x = F.relu(self.conv2(x))\n        x = x.view(in_size, -1)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        x = F.log_softmax(x, dim=1) # 按行进行\n        return x        ","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = MNIST_NET().to(Device)\noptimizer = optim.Adam(model.parameters())","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(model, device, train_loader, optimizer, epoch):\n    model.train()  # 告诉网络现在要训练，会影响BN 以及 dropout\n    for batch_idx, datas in enumerate(train_loader):\n        data, target = datas\n        data, target = data.to(device), target.to(device)\n        \n        optimizer.zero_grad() # 梯度归零\n        output = model(data)  # 将数据送入网络\n        loss = F.nll_loss(output, target)\n        loss.backward()\n        optimizer.step()\n        \n        if(batch_idx + 1) % 30 == 0:\n            print(\"Train Epoch: {} [{} / {} ({:.0f}%)]\\tLoss:{:.6f}\".format(\n            epoch, batch_idx * len(data), len(train_loader.dataset),\n            100. * batch_idx / len(train_loader), loss.item()))","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def test(model , device, test_loader):\n    model.eval()\n    test_loss = 0.\n    correct = 0. \n    with torch.no_grad():  # 不进行梯度下降\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            test_loss += F.nll_loss(output, target, reduction='sum').item() # 将同一batch的损失相加\n            pred = output.max(1, keepdim=True)[1]  # 返回最大值的位置\n            correct += pred.eq(target.view_as(pred)).sum().item()\n            \n    test_loss /= len(test_loader.dataset)\n    print(\"\\nTest set:Avg loss:{:.4f}, Acc:{}/{} ({:.0f}%)\\n\".format(\n        test_loss, correct, len(test_loader.dataset),\n        100. * correct / len(test_loader.dataset)))","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for epoch in range(1, Epochs + 1):\n    train(model, Device, train_loader, optimizer, epoch)\n    test(model, Device, test_loader)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}