# 基于RGB三通道的LapSRN超分辨算法

## 利用更多数据训练，迭代较少

![Snipaste_2020-10-09_22-34-55](https://tva3.sinaimg.cn/large/005tpOh1ly1gjjghkuemcj30r00ingr7.jpg)

- 训练集：2112张

- 测试集：518张

- 图像像素分别为：128pixel、256pixel、512pixel

- 训练由于数据集较大，只能训练**115 epochs**
- 训练结果：

![Snipaste_2020-10-09_22-44-23](https://tva4.sinaimg.cn/large/005tpOh1ly1gjjgr8w4btj30od0e8gn9.jpg)

<center><font color='red'>在101epoch后，loss 趋近于 0.0334</font></center>

![Snipaste_2020-10-09_22-49-42](https://tvax2.sinaimg.cn/large/005tpOh1ly1gjjgwtxb0kj30cq08amy8.jpg)

<center><font color='red'>PSNR1 为放大2倍的PSNR，PSNR2为放大4倍的PSNR</font></center>

![Snipaste_2020-10-09_23-08-54](https://tva2.sinaimg.cn/large/005tpOh1ly1gjjhgs1kf8j30ta0kk43g.jpg)

**结果分析**：在两倍超分辨时，虽然在PSNR上表现不错，但是从视觉上看，超分结果对**红色**通道的还原严重不足，在绿色通道上虽然比红色好，但是放大后看细节，图像过于平滑，缺少了很多的细节。

![Snipaste_2020-10-09_23-23-12](https://tva3.sinaimg.cn/large/005tpOh1ly1gjjhvmdbjej30vo0m00y4.jpg)

**结果分析**：在四倍超分辨时，不仅在PSNR上表现不如2倍时，在还原时，红色严重的缺失，且细节的还原也不够



## 利用更少数据训练，迭代更多

- 训练集160张
- 测试集32张
- 训练**1700epochs**
- 训练结果：

![Snipaste_2020-10-09_23-27-15](https://tva1.sinaimg.cn/large/005tpOh1ly1gjjhzuhiowj30oa0cz0tp.jpg)

<center><font color='red'>在400epoch后，loss 趋近于 0.0363</font></center>

![Snipaste_2020-10-09_23-29-57](https://tvax2.sinaimg.cn/large/005tpOh1ly1gjji2o8i0cj30od0epmyc.jpg)

<center><font color='red'>PSNR1 为放大2倍的PSNR，PSNR2为放大4倍的PSNR</font></center>

**总结**：虽然在少量的数据集上可以进行更多的迭代，但是，在还原上并没有达到更好，反而在2倍超分辨是更加的差（视觉效果和上面的大同小异，在压缩包里）

## 不同训练方法的结果比较

![Snipaste_2020-10-09_23-46-41](https://tvax1.sinaimg.cn/large/005tpOh1ly1gjjik3oov3j31310kr0z5.jpg)

<center><font color='red'>超分辨2倍的不同方法比较</font></center>

![Snipaste_2020-10-09_23-55-08](https://tvax4.sinaimg.cn/large/005tpOh1ly1gjjisuoy9mj30ws0hd0x8.jpg)

<center><font color='red'>超分辨4倍的不同方法比较</font></center>

# 一段有意义的代码

> 以往做的都是单通道的，所以在测试上有差不多的代码可以用。但是，这次直接进行单通道的输出，如何把输出进行可视化是重要的，利用以下代码即可保存模型

```python
HR_2_r, HR_4_r = model(LR_r)  # 模型输出
torchvision.utils.save_image(HR_2_r.data,'%d.tif'% (1), padding=0) # 输出保存
```

当然，还有不利用`torchvision`的方法：

```python
HR_2_r, HR_4_r = model(LR_r)
out_HR_2_r = HR_2_r.squeeze(0).detach().numpy()
out_HR_2_r *= 255.0
out_HR_2_r = out_HR_2_r.clip(0, 255)
out_HR_2_r = np.transpose(out_HR_2_r,(1, 2, 0))
out_HR_2_r = Image.fromarray(np.uint8(out_HR_2_r), mode='RGB')
out_HR_2_r.save('./LapSRN_RGB_ID44_Epoch400_X2_Less.tif')
```

上面代码整理到`test.py`中



# 总结

- 从目前的训练结果看，三通道的数据集直接进行训练的结果比单通道训练后合成的结果差，特别是在4倍这样更高的倍数时。我认为的原因有：
  - 红色、绿色在不同通道上分布不均
  - 直接三通道训练时，第三通道没有信息
- 之后尝试**数据增强**后的三通道训练，以及利用不用的网络训练