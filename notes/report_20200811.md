# 解决GPU算力问题

经过网上大量的搜罗，我整理了适合的深度学习解决方案。

## Colab

Google Colab提供了免费K80的GPU，通过Google Drive就可以很好的白嫖一波了，👉[配置教程](https://www.cnblogs.com/zgqcn/p/11186406.html)

通过下载**Google备份与同步**到本地端，就可以实现数据的同步，保证了较大量的数据集在云端和本地端直接的无差错传输

<img src="https://tvax4.sinaimg.cn/large/005tpOh1gy1ghmunxq0opj30dk0jnjta.jpg" alt="Snipaste_2020-08-11_14-21-27" style="zoom:80%;" />

通过这一同步，在本地端修改代码，可以在1分钟之内就同步到云端，方便与训练与修改。

❌但是，K80 GPU的算力较弱，且colab的连接不稳定，适用于对代码进行调试，不适于长时间的训练。

## Kaggle

Kaggle提供免费访问内核中的Nvidia Tesla P100，相比于K80，算力提升了太多了。我在colab训练10小时最好80epochs，在kaggle上，训练不到1小时就可以80epochs，真实太感动了😂，配置如下：

### 创建账号

[Kaggle.com](https://www.kaggle.com/)

### 创建NoteBook

![Snipaste_2020-08-11_14-35-13](https://tva3.sinaimg.cn/large/005tpOh1gy1ghmv25udfsj31hc0im410.jpg)

![Snipaste_2020-08-11_14-36-58](https://tvax3.sinaimg.cn/large/005tpOh1gy1ghmv3ysfiqj31h50p3mzv.jpg)

**建议**：创建时可以不选GPU，直接None就行了，否则，创建后就开始计时了，GPU一周可以使用39小时（当然可以多多创建账号）

### 配置notebook

![Snipaste_2020-08-11_14-41-30](https://tva2.sinaimg.cn/large/005tpOh1gy1ghmv8px62tj31hc0j7439.jpg)

1. 上传代码及数据集要求压缩包上传，上传后Add进去就好了

<img src="https://tvax3.sinaimg.cn/large/005tpOh1gy1ghmvag993mj30mw0g5q3w.jpg" alt="Snipaste_2020-08-11_14-43-09" style="zoom:70%;" />

2. 写控制命令时，只需要复制路径就可以获得train.py的位置

<img src="https://tva1.sinaimg.cn/large/005tpOh1gy1ghmvd09d8pj309807omxe.jpg" alt="Snipaste_2020-08-11_14-45-36" style="zoom:80%;" />

3. 训练结果保存的位置应该为`/kaggle/working/...`

<img src="https://tvax4.sinaimg.cn/large/005tpOh1gy1ghmvervhyaj30mq03h3yv.jpg" alt="Snipaste_2020-08-11_14-47-20" style="zoom:80%;" />

4. 数据集与代码管理

<img src="https://tvax4.sinaimg.cn/large/005tpOh1gy1ghmvk4uhq9j31h00o0dk4.jpg" alt="Snipaste_2020-08-11_14-52-21" style="zoom:70%;" />



通过这两大平台的结合使用，可以方便的进行深度学习了.



# LapSRN训练

训练集：160张

测试集：32张

训练参数：`batchSize = 8, learning rate = 1e-3`

## R通道训练

![Snipaste_2020-08-14_15-55-44](https://tva3.sinaimg.cn/large/005tpOh1gy1ghqe9h6zzfj31360enta8.jpg)

**Loss Function**在200代后接近于收敛，PSNR也在200代收敛（每10代进行测试，所以坐标20实为第200代）

## G通道训练

![Snipaste_2020-08-14_16-03-20](https://tvax1.sinaimg.cn/large/005tpOh1gy1ghqegt4wybj312x0eidh7.jpg)

在G通道上训练了**1000代**，**Loss Function**在200代后接近于收敛

## 视觉效果

### 放大4倍

![Snipaste_2020-08-14_16-10-29](https://tva3.sinaimg.cn/large/005tpOh1gy1ghqeo63i3lj31hc0q17et.jpg)

![Snipaste_2020-08-14_16-12-48](https://tva4.sinaimg.cn/large/005tpOh1gy1ghqeqm9cuhj31hc0qjwn6.jpg)

**通过对比原理的2048\*2048的HR高分辨图像，4倍超分辨后：**

1. 红色部分（弹力纤维）能够更加的明显

2. 绿色部分（胶原纤维）的超分辨有明显的颗粒感，不够平滑

### 放大2倍

![Snipaste_2020-08-14_16-18-04](https://tvax3.sinaimg.cn/large/005tpOh1gy1ghqew42inxj31hc0q412o.jpg)

![Snipaste_2020-08-14_16-20-06](https://tva4.sinaimg.cn/large/005tpOh1gy1ghqey8epytj31hc0qg45u.jpg)

在超分辨效果上，出现了和4倍相同的效果。